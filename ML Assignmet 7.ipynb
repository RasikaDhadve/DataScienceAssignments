{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "  1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?\nAns: A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. The target function is essentially the formula that an algorithm feeds data to in order to calculate predictions.\nAnalysing the massive amounts of data related to its given problem, an AI derives understanding of previously unspecified rules by detecting consistencies in the data. The observations of inherent rules about how the studied subject operates inform the AI on how to process future data that does not include an output by applying this previously unknown function\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\nAns: Predictive modelling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data.\nIt works by analysing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\nThe three main types of descriptive studies are Case studies, Naturalistic observation, and Surveys.\nSome examples of descriptive research are: A specialty food group launching a new range of barbecue rubs would like to understand what Flavors of rubs are favoured by different people.\nCase Studies are a type of observational research that involve a thorough descriptive analysis of a single individual, group, or event. There is no single way to conduct a case study so researchers use a range of methods from unstructured interviewing to direct observation.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters.\nAns: Assessing the efficiency of a classification model involves evaluating how well the model performs in categorizing instances into different classes. There are several measurement parameters that provide insights into different aspects of the model's performance. Here are the key measurement parameters commonly used to assess the efficiency of a classification model:\nAccuracy: Accuracy is a simple and widely used metric that measures the overall correctness of the model's predictions. It is the ratio of correctly predicted instances to the total instances in the dataset.\n   Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\nPrecision: Precision focuses on the accuracy of the positive predictions made by the model. It answers the question: Of all instances predicted as positive, how many were actually positive?\n   Precision = (True Positives) / (True Positives + False Positives)\n\n   Precision is particularly useful when the cost of false positives is high (e.g., in medical diagnoses) and you want to minimize incorrect positive predictions.\nRecall (Sensitivity or True Positive Rate): Recall measures the ability of the model to correctly identify all relevant instances. It answers the question: Of all actual positive instances, how many were correctly predicted as positive?\n   Recall = (True Positives) / (True Positives + False Negatives)\n   Recall is crucial when missing actual positive instances is more concerning (e.g., in detecting fraud or diseases).\nF1-Score: The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure that takes into account both false positives and false negatives.\n   F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n   The F1-Score is useful when you want to find a balance between precision and recall.\nSpecificity (True Negative Rate): Specificity measures the ability of the model to correctly identify negative instances.\n   Specificity = (True Negatives) / (True Negatives + False Positives)\n   Specificity is important in scenarios where correctly identifying negative instances is critical.\n ROC Curve (Receiver Operating Characteristic Curve): The ROC curve is a graphical representation that illustrates the trade-off between the true positive rate (recall) and the false positive rate. AUC (Area Under the Curve) of the ROC curve quantifies the model's ability to distinguish between different classes.\nConfusion Matrix: A confusion matrix is a tabular representation that shows the number of true positives, true negatives, false positives, and false negatives. It's a useful way to visualize the performance of a classification model.\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "4. \n      i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\nIn machine learning, underfitting refers to a situation where a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and testing/validation datasets. An underfit model fails to generalize well to new, unseen data points, leading to low accuracy and unsatisfactory predictive power.\nThe most common reason for underfitting is model complexity that is too low to capture the complexities present in the data.\nThis often happens when:\n-The Model is Too Simple\n-Insufficient Training\n-Limited Features\n-Over-regularization\n-Small Training Dataset\n-Ignoring Important Features\n-Data Noise\n     ii. What does it mean to overfit? When is it going to happen?\nOverfitting occurs when a model learns the training data too well, to the point that it captures the noise and random fluctuations in the data rather than the underlying patterns. As a result, an overfit model performs exceptionally well on the training data but generalizes poorly to new, unseen data. Overfitting can lead to reduced predictive power and poor performance on validation or test datasets.\nOverfitting is more likely to happen when:\n-Model Complexity is Too High\n-Insufficient Training Data\n-Too Many Features\n-Overemphasis on Noise\n-Not Enough Regularization\n-Overfitting in Deep Learning\n\n    iii. In the sense of model fitting, explain the bias-variance trade-off.\nThe bias-variance trade-off is a fundamental concept in machine learning that highlights the relationship between two sources of error that affect a model's performance: bias and variance. Achieving a balance between bias and variance is crucial for creating models that generalize well to new, unseen data.\n-Bias\n-Variance\n-High Bias, Low Variance\n-Low Bias, High Variance\n-Balanced Bias and Variance\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n Building a machine learning model is not enough to get the right predictions, as you have to check the accuracy and need to validate the same to ensure get the precise results. And validating the model will improve the performance of the ML model. Some ways of boosting the efficiency of a learning model are mentioned below:\nAdd more Data Samples\nLook at the problem differently: Looking at the problem from a new perspective can add valuable information to your model and help you uncover hidden relationships between the story variables. Asking different questions may lead to better results and, eventually, better accuracy.\nAdding Context to Data: More context can always lead to a better understanding of the problem and, eventually, better performance of the model. Imagine we are selling a car, a BMW. That alone doesn’t give us much information about the car. But, if we add the color, model and distance travelled, then you’ll start to have a better picture of the car and its possible value.\nFinetuning our hyperparameter: to get the answer, we will need to do some trial and error until you reach your answer.\nTrain our model using cross-validation\nExperimenting with different Algorithms.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?\nIn case of supervised learning, it is mostly done by measuring the performance metrics such as accuracy, precision, recall, AUC, etc. on the training set and the holdout sets whereas for Unsupervised Learning it is different. Since there is no pre-evidence or records for patterns, we cannot directly compute the accuracy by comparing actual and predicted outputs but there exist many evaluation metrics to measure the performance of unsupervised learning algorithms after the training process.\nSome of them are-\nClustering - Jaccard similarity index, Rand Index, Purity, Silhouette measure, Sum of squared errors, etc.\nAssociation rule mining – Lift, Confidence\nTime series analysis – Root mean square error, mean absolute error, mean absolute percentage error, etc.\nAutoencoders - Reconstruction errors\nNatural Language processing (like sentiment analysis and text clustering) – Comparing the correlation between natural words after converting them to numerical vectors.\nPrincipal component analysis – Reconstruction error, Scree plot\nGenerative adversarial networks – Discriminator functions\nRecurrent neural networks and LSTM (In numerical series) – Root mean square error, mean absolute error, mean absolute percentage error, etc.\nRecurrent neural networks and LSTM (In semantic series) - Word to vector correlation\nAnomaly detection (like DBSCAN, OPTICS) – Cohesion, Separation, Sum of squared errors, etc.\nExpectation/ Maximization problems – Log-likelihood\nSurvival analysis (Cox model 1) – Simple hazard ration, R Squared Survival analysis (Cox model 2) – Two group hazard ratio and brier score, Log-rank test, Somers’ rank correlation, Time-dependent ROC – AUC, Power validation, etc.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n Categorical Data is the data that generally takes a limited number of possible values. Also, the data in the category need not be numerical, it can be textual in nature. All machine learning models are some kind of mathematical model that need numbers to work with. This is one of the primary reasons we need to pre-process the categorical data before we can feed it to machine learning models.\nIf a categorical target variable needs to be encoded for a classification predictive modelling problem, then the Label Encoder class can be used.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\npredictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data. It works by analysing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\nClassification is the process of identifying the category or class label of the new observation to which it belongs. Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction.\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n         i. Accurate estimates – 15 cancerous, 75 benign\n         ii. Wrong predictions – 3 cancerous, 7 benign\n                Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n- Accurate estimates:\n  - True Positives (TP) = 15 (cancerous correctly predicted)\n  - True Negatives (TN) = 75 (benign correctly predicted)\n- Wrong predictions:\n  - False Positives (FP) = 7 (benign incorrectly predicted as cancerous)\n  - False Negatives (FN) = 3 (cancerous incorrectly predicted as benign)\nTotal number of samples = TP + TN + FP + FN = 15 + 75 + 7 + 3 = 100\n1. Error Rate:\n   Error Rate = (FP + FN) / Total Number of Samples\n   Error Rate = (7 + 3) / 100 = 0.1 or 10%\n\n2. Kappa Value:\n   Kappa Value is a measure of agreement between the actual classifications and the model's predictions, considering the agreement that could occur by chance.\n   Kappa Value = (Accuracy - Chance Agreement) / (1 - Chance Agreement)\n      First, calculate the Accuracy:\n   Accuracy = (TP + TN) / Total Number of Samples = (15 + 75) / 100 = 0.9 or 90%\n      Next, calculate the Chance Agreement:\n   Chance Agreement = (TP + FP) * (TP + FN) / Total Number of Samples^2 + (TN + FP) * (TN + FN) / Total Number of Samples^2\n      Chance Agreement = (15 + 7) * (15 + 3) / 100^2 + (75 + 7) * (75 + 3) / 100^2\n   Chance Agreement = 0.22\n      Now calculate the Kappa Value:\n   Kappa Value = (0.9 - 0.22) / (1 - 0.22) = 0.755\n3. Sensitivity (True Positive Rate):\n   Sensitivity = TP / (TP + FN) = 15 / (15 + 3) = 0.8333 or 83.33%\n4. Precision:\n   Precision = TP / (TP + FP) = 15 / (15 + 7) = 0.6818 or 68.18%\n5. F-Measure (F1-Score):\n   F1-Score = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n   F1-Score = 2 * (0.6818 * 0.8333) / (0.6818 + 0.8333) = 0.7504 or 75.04%\nTo summarize:\n- Error Rate: 10%\n- Kappa Value: 0.755\n- Sensitivity: 83.33%\n- Precision: 68.18%\n- F-Measure: 75.04%\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "10. Make quick notes on:\n         1. The process of holding out\n The hold-out method for training machine learning model is the process of splitting the data in different splits and using one split for training the model and other splits for validating and testing the models. The hold-out method is used for both model evaluation and model selection.\n         2. Cross-validation by tenfold\n 10-fold cross validation would perform the fitting procedure a total of ten times, with each fit being performed on a training set consisting of 90% of the total training set selected at random, with the remaining 10% used as a hold out set for validation.\n         3. Adjusting the parameters\n A fancy name for training: the selection of parameter values, which are optimal in some desired sense (eg. minimize an objective function you choose over a dataset you choose). The parameters are the weights and biases of the network\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "11. Define the following terms: \n         1. Purity vs. Silhouette width\n Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster.\nThe silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n         2. Boosting vs. Bagging\n Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data.\nBoosting is an iterative technique which adjusts the weight of an observation based on the last classification.\n         3. The eager learner vs. the lazy learner\n A lazy learner delays abstracting from the data until it is asked to make a prediction.\nwhile an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}